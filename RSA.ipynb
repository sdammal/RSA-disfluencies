{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create Data From Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Dataset:\n",
    "    #https://groups.inf.ed.ac.uk/switchboard/\n",
    "    #Acquired by emailing the authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import operator\n",
    "import os\n",
    "\n",
    "#Share path to dataset\n",
    "path = 'swb/swb_nxt/xml/'\n",
    "\n",
    "L = os.listdir(path+'syntax/')\n",
    "L.sort()\n",
    "\n",
    "L = [x.split('.')[:2] for x in L]\n",
    "\n",
    "L2 = list(set([x[0] for x in L]))\n",
    "L2.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# L2 is the list of all speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Parsing the XML file and recording the word, pos tag and durations\n",
    "TERMINALS = {}\n",
    "for i in L2:\n",
    "    \n",
    "    xmlfile = i\n",
    "    xmlfileA = i + '.A.'\n",
    "    xmlfileB = i + '.B.'\n",
    "    \n",
    "    TERMINALS[xmlfile+'A'] = {}\n",
    "    TERMINALS[xmlfile+'B'] = {}\n",
    "\n",
    "    #XML root name of speaker A\n",
    "    terminalsA = ET.parse(path+'terminals/'+xmlfileA+'terminals.xml')\n",
    "    terRootA = terminalsA.getroot()\n",
    "    \n",
    "    #XML root name of speaker B\n",
    "    terminalsB = ET.parse(path+'terminals/'+xmlfileB+'terminals.xml')\n",
    "    terRootB = terminalsB.getroot()\n",
    "    \n",
    "    nite = '{http://nite.sourceforge.net/}'\n",
    "\n",
    "    \n",
    "    set_id = 's'\n",
    "    #Parse children\n",
    "    for child in terRootA: \n",
    "        \n",
    "        #If child is word\n",
    "        if child.tag == 'word':\n",
    "            \n",
    "            if child.attrib[nite+'end'] not in ('non-aligned','n/a') and child.attrib[nite+'start'] not in ('non-aligned','n/a'):\n",
    "                #Store information of word\n",
    "                TERMINALS[xmlfile+'A'][child.attrib[nite+'id']] = [child.attrib['orth'].lower(), child.attrib['pos'],float(child.attrib[nite+'start']),float(child.attrib[nite+'end']),\"fluent\"]\n",
    "            else:\n",
    "                #Misaligned durations marked as -1, and ignored later\n",
    "                TERMINALS[xmlfile+'A'][child.attrib[nite+'id']] = [child.attrib['orth'].lower(), child.attrib['pos'],-1,-1,\"fluent\"]\n",
    "\n",
    "        #If child is not a word, store is as special (marked with $)\n",
    "        else:\n",
    "            TERMINALS[xmlfile+'A'][child.attrib[nite+'id']] = ['$' + child.tag, \"$\",-1,-1,\"$\"]\n",
    "\n",
    "    #Same procedure is as A is followed for B\n",
    "    for child in terRootB:\n",
    "        if child.tag == 'word':\n",
    "            if child.attrib[nite+'end'] not in ('non-aligned','n/a') and child.attrib[nite+'start'] not in ('non-aligned','n/a'):\n",
    "                TERMINALS[xmlfile+'B'][child.attrib[nite+'id']] = [child.attrib['orth'].lower(), child.attrib['pos'],float(child.attrib[nite+'start']),float(child.attrib[nite+'end']),\"fluent\"]\n",
    "            else:\n",
    "                TERMINALS[xmlfile+'B'][child.attrib[nite+'id']] = [child.attrib['orth'].lower(), child.attrib['pos'],-1,-1,\"fluent\"]\n",
    "\n",
    "        else:\n",
    "            TERMINALS[xmlfile+'B'][child.attrib[nite+'id']] = ['$' + child.tag, \"$\",-1,-1,\"$\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Store disfluencies\n",
    "DISFLUENCIES = {}\n",
    "REPARAS = {}\n",
    "for i in L2:\n",
    "    \n",
    "    xmlfileA = i+'.A.'\n",
    "    xmlfileB = i+'.B.'\n",
    "    xmlfile = i\n",
    "    \n",
    "\n",
    "    disfluencyA = ET.parse(path+'disfluency/'+xmlfileA+'disfluency.xml')\n",
    "    dRootA = disfluencyA.getroot()\n",
    "    \n",
    "    disfluencyB = ET.parse(path+'disfluency/'+xmlfileB+'disfluency.xml')\n",
    "    dRootB = disfluencyB.getroot()\n",
    "    \n",
    "    nite = '{http://nite.sourceforge.net/}'\n",
    "    \n",
    "    \n",
    "    reparas = [] #Sorted pair of reparandum and repair\n",
    "    rdm = {} #Store reparandums in this dict\n",
    "    rp = {} #Store repair in this dict\n",
    "    for dis in dRootA:\n",
    "        \n",
    "        if dis.tag == 'disfluency':\n",
    "            a = []\n",
    "            b = []\n",
    "            for child in dis:\n",
    "\n",
    "                if child.tag == 'reparandum':\n",
    "                    for r in child.iter():\n",
    "                        if r.tag == nite+'child':\n",
    "                            rdm[r.attrib['href'].split('#')[1][3:-1]] = 1\n",
    "                            a.append(r.attrib['href'].split('#')[1][3:-1])\n",
    "\n",
    "                if child.tag == 'repair':\n",
    "                    for r in child.iter():\n",
    "                        if r.tag == 'disfluency':\n",
    "                            for reparand in r:\n",
    "                                if reparand.tag == 'reparandum':\n",
    "                                    for r_child in reparand.iter():\n",
    "                                        if 'href' in r_child.attrib:\n",
    "                                            rdm[r_child.attrib['href'].split('#')[1][3:-1]] = 1\n",
    "                                            a.append(r_child.attrib['href'].split('#')[1][3:-1])\n",
    "\n",
    "                    for r in child.iter():\n",
    "                        if r.tag == nite+'child':\n",
    "                            if r.attrib['href'].split('#')[1][3:-1] not in rdm:\n",
    "                                rp[r.attrib['href'].split('#')[1][3:-1]] = 1\n",
    "                                b.append(r.attrib['href'].split('#')[1][3:-1])\n",
    "            \n",
    "            #Store sorted pairs of reparandum and repair\n",
    "            reparas_ = []\n",
    "            q = sorted(a, key=lambda x: float(x.split('_')[0][1:]) + 0.0001* float(x.split('_')[1]))\n",
    "            if len(q) > 0:\n",
    "                reparas_.append(q)\n",
    "                \n",
    "            q = sorted(b, key=lambda x: float(x.split('_')[0][1:]) + 0.0001* float(x.split('_')[1]))\n",
    "            if len(q) > 0:\n",
    "                reparas_.append(q)\n",
    "                \n",
    "            if len(reparas_) > 0:\n",
    "                reparas.append(reparas_)\n",
    "    \n",
    "    REPARAS[xmlfileA] = reparas #Update the reparandums values into the REPARAS dict for speaker A\n",
    "    \n",
    "    reparas = [] #Sorted pair of reparandum and repair\n",
    "                            \n",
    "    for dis in dRootB:\n",
    "        \n",
    "        if dis.tag == 'disfluency':\n",
    "            a = []\n",
    "            b = []\n",
    "            for child in dis:\n",
    "                if child.tag == 'reparandum':\n",
    "                    for r in child.iter():\n",
    "                        if r.tag == nite+'child':\n",
    "                            rdm[r.attrib['href'].split('#')[1][3:-1]] = 1\n",
    "                            a.append(r.attrib['href'].split('#')[1][3:-1])\n",
    "\n",
    "                if child.tag == 'repair':\n",
    "\n",
    "                    for r in child.iter():\n",
    "                        if r.tag == 'disfluency':\n",
    "                            for reparand in r:\n",
    "                                if reparand.tag == 'reparandum':\n",
    "                                    for r_child in reparand.iter():\n",
    "                                        if 'href' in r_child.attrib:\n",
    "                                            rdm[r_child.attrib['href'].split('#')[1][3:-1]] = 1\n",
    "                                            a.append(r_child.attrib['href'].split('#')[1][3:-1])\n",
    "\n",
    "                    for r in child.iter():\n",
    "                        if r.tag == nite+'child':\n",
    "                            if r.attrib['href'].split('#')[1][3:-1] not in rdm:\n",
    "                                rp[r.attrib['href'].split('#')[1][3:-1]] = 1\n",
    "                                b.append(r.attrib['href'].split('#')[1][3:-1])\n",
    "\n",
    "            #Store sorted pairs of reparandum and repair\n",
    "            reparas_ = []\n",
    "            q = sorted(a, key=lambda x: float(x.split('_')[0][1:]) + 0.0001* float(x.split('_')[1]))\n",
    "            if len(q) > 0:\n",
    "                reparas_.append(q)\n",
    "                \n",
    "            q = sorted(b, key=lambda x: float(x.split('_')[0][1:]) + 0.0001* float(x.split('_')[1]))\n",
    "            if len(q) > 0:\n",
    "                reparas_.append(q)\n",
    "                \n",
    "            if len(reparas_)> 0:\n",
    "                reparas.append(reparas_)\n",
    "                    \n",
    "    REPARAS[xmlfileB] = reparas #Update the reparandums values into the REPARAS dict for speaker B\n",
    "    \n",
    "    DISFLUENCIES[xmlfile] = {} #Store DISFLUENCIES for conversations\n",
    "    DISFLUENCIES[xmlfile]['repair'] = rp.keys() #Store all repairs for conversation\n",
    "    DISFLUENCIES[xmlfile]['reparandum'] = rdm.keys() #Store all reparandums for conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create a label for each word, whether it is repair or reparandum\n",
    "MARKER = {}\n",
    "for i in DISFLUENCIES.keys():\n",
    "    MARKER[i] = {}\n",
    "    for k in DISFLUENCIES[i]['repair']:\n",
    "        MARKER[i][k] = 'rp'\n",
    "        \n",
    "    for k in DISFLUENCIES[i]['reparandum']:\n",
    "        MARKER[i][k] = 'rpdm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create list of all contractions i.e. words like n't (don't, hasn't) or 's (it's, she's).\n",
    "#This is important because GPT-2 needs contractions to not have spaces before it\n",
    "        #For example, sentences should be structured as It's raining rather than It 's raining\n",
    "contractions = []\n",
    "for i in TERMINALS.keys():\n",
    "    for k in TERMINALS[i]:\n",
    "        if k in MARKER[i[:-1]]:\n",
    "            TERMINALS[i][k][-1] = MARKER[i[:-1]][k]\n",
    "        if TERMINALS[i][k][-1] == 'fluent' and TERMINALS[i][k][1] == 'UH':\n",
    "            TERMINALS[i][k][-1] = 'filler'\n",
    "            \n",
    "        if \"'\" in TERMINALS[i][k][0]:\n",
    "            contractions.append(TERMINALS[i][k][0])\n",
    "\n",
    "contractions = set(contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create a dictionary for each sentence, which contains all the word tags\n",
    "SETS = {}\n",
    "\n",
    "key = ''\n",
    "\n",
    "for speaker in TERMINALS:\n",
    "    \n",
    "    Q = list(TERMINALS[speaker].keys())\n",
    "\n",
    "    key = speaker + '_' + Q[0].split('_')[0]\n",
    "    k = [Q[0]]\n",
    "    for i in range(0,len(Q)-1):\n",
    "\n",
    "        if Q[i].split('_')[0] == Q[i+1].split('_')[0]:\n",
    "            k.append(Q[i+1])\n",
    "\n",
    "        else:\n",
    "            SETS[key] = k\n",
    "            key = speaker + '_' + Q[i+1].split('_')[0]\n",
    "            k = [Q[i+1]]\n",
    "\n",
    "    if key not in SETS:\n",
    "        SETS[key] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#GET SPEAKER FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nite = '{http://nite.sourceforge.net/}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Capture speaker features: sex, date of birth, dialect\n",
    "SPEAKER = {}\n",
    "terminals = ET.parse(\"swb/swb_nxt/xml/corpus-resources/speakers.xml\") \n",
    "ter = terminals.getroot()\n",
    "\n",
    "for child in ter: \n",
    "    #Match the speaker number and their features\n",
    "    SPEAKER[child.attrib[nite+'id']] = [child.attrib['sex'],child.attrib['dob'],child.attrib['dialect']]    \n",
    "\n",
    "terminals = ET.parse(\"swb/swb_nxt/xml/corpus-resources/dialogues.xml\") \n",
    "ter = terminals.getroot()\n",
    "\n",
    "for child in ter: \n",
    "    for spks in child:\n",
    "        if spks.attrib['role'] != 'topic':\n",
    "            #Match the speaker number and conversation number \n",
    "            SPEAKER[child.attrib['swbdid']+spks.attrib['role']]=SPEAKER[spks.attrib['href'].split('(')[1][:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Examine Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create tags for each class\n",
    "TAGS = {}\n",
    "TAGS['$'] = \"\"\n",
    "TAGS['fluent'] = \"F\"\n",
    "TAGS['filler'] = \"D\"\n",
    "TAGS['rpdm'] = \"O\" #O is for reparandum\n",
    "TAGS['rp'] = \"P\" #P is for repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Count tags for each class\n",
    "cTAGS = {}\n",
    "cTAGS['F'] = 0\n",
    "cTAGS['D'] = 0\n",
    "cTAGS['O'] = 0 #O is for reparandum\n",
    "cTAGS['P'] = 0 #P is for repair\n",
    "cTAGS[''] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Ls = [] #Create List with TAGS\n",
    "Ws = [] #Create List with WORDS\n",
    "sent_keys = list(SETS.keys())\n",
    "for k in sent_keys:\n",
    "    ws = []\n",
    "    s = \"\"\n",
    "    for j in SETS[k]:\n",
    "        q = TAGS[TERMINALS[k.split('_')[0]][j][-1]]\n",
    "        s = s + q\n",
    "        ws.append(TERMINALS[k.split('_')[0]][j][0])\n",
    "        cTAGS[q] = cTAGS[q] + 1\n",
    "                    \n",
    "    Ls.append(s)\n",
    "    Ws.append(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Count lengths of each sentence using Ls: The list of TAGS\n",
    "LENS = []\n",
    "for i in Ls:\n",
    "    q = i.count('F') + i.count('P') #Count number of fluent and repair words\n",
    "    if q!=0:\n",
    "        LENS.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#% of sentences less than length 20 \n",
    "sum([x < 20 for x in LENS])/len(LENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#% of sentences less than length 20 and more than 6\n",
    "sum([(x < 20 and x > 6)*1 for x in LENS])/len(LENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Plot sentences and length\n",
    "import matplotlib.pyplot as plt\n",
    "ax = sns.histplot(LENS,bins=40)\n",
    "ax.set(xlabel='length', ylabel='count')\n",
    "plt.savefig('filename.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Prior calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## CALCULATE PRIOR VALUES FROM THIS:\n",
    "\n",
    "#% of fluent/repair words \n",
    "(cTAGS['F'] + repair_count)/(cTAGS['F']+cTAGS['D']+reparandum_count+repair_count)\n",
    "\n",
    "#% of disfluent tags\n",
    "cTAGS['D']/(cTAGS['F']+cTAGS['D']+reparandum_count+repair_count)\n",
    "\n",
    "#Count number of instances of reparandum and repair occurring\n",
    "reparandum_count = 0\n",
    "for i in set(Ls):\n",
    "    reparandum_count = reparandum_count + i.count('OP')*COUNT_DICT[i]\n",
    "    \n",
    "repair_count = 0\n",
    "for i in set(Ls):\n",
    "    repair_count = repair_count + i.count('P')*COUNT_DICT[i]\n",
    "\n",
    "#% of reparandum tags\n",
    "(reparandum_count)/(cTAGS['F']+cTAGS['D']+reparandum_count+repair_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "COUNT_DICT = Counter(Ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#COUNT NUMBER OF FLUENT, FILLERS and REPARANDUMS\n",
    "l = 6\n",
    "r = 20\n",
    "\n",
    "count = 0\n",
    "for i in set(Ls):\n",
    "    if i.count('D') == 1 and i.count('O') == 0 and i.count('P') == 0:\n",
    "        if len(i) < r and len(i) > l:\n",
    "            count = count + COUNT_DICT[i]\n",
    "\n",
    "print(\"Filler\",count) #Only filler\n",
    "\n",
    "count = 0\n",
    "for i in set(Ls):\n",
    "    \n",
    "    if i.count('D') == 0 and i.count('O') < 3 and i.count('P') == 1:\n",
    "        if len(i) < r and len(i) > l:\n",
    "            count = count + COUNT_DICT[i]\n",
    "\n",
    "print(\"Rpdm\",count) #only rpdm\n",
    "\n",
    "count = 0\n",
    "for i in set(Ls):\n",
    "    if i.count('D') == 0 and i.count('O') == 0 and i.count('P') == 0:\n",
    "        if len(i) < r and len(i) > l:\n",
    "                count = count + COUNT_DICT[i]\n",
    "\n",
    "print(\"Fluent\",count) #Only fluent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create DataFrame X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Start creating dataset X: appending information on sentence number, class, sentence TAGS and length\n",
    "    #Example for sentence TAGS:\n",
    "#         DFFFFF for a sentence with filler in the beginning and 5 fluent words following after that\n",
    "\n",
    "X = []\n",
    "\n",
    "l = 6\n",
    "r = 20\n",
    "\n",
    "for j in range(0,len(Ls)):\n",
    "    \n",
    "    i = Ls[j]\n",
    "\n",
    "    #Filler\n",
    "    if i.count('D') == 1 and i.count('O') == 0 and i.count('P') == 0:\n",
    "        if len(i) < r and len(i) > l:\n",
    "            X.append(['filler',sent_keys[j],i,i.count('F')])\n",
    "\n",
    "    #RPDM\n",
    "    if i.count('D') == 0 and i.count('O') < 3 and i.count('P') == 1:\n",
    "        if len(i) < r and len(i) > l:\n",
    "            X.append(['rpdm',sent_keys[j],i,i.count('F') + i.count('P')])\n",
    "\n",
    "\n",
    "\n",
    "    #Only fluent\n",
    "    if i.count('D') == 0 and i.count('O') == 0 and i.count('P') == 0:\n",
    "        if len(i) < r and len(i) > l:\n",
    "            X.append(['fluent',sent_keys[j],i,i.count('F')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Add column names\n",
    "X = pd.DataFrame(X,columns=['class','sentence_label','type','length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X['prior'] = [PRIOR[x] for x in X['class'].values]\n",
    "X['duration'] = [DURATION[x] for x in X['class'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Duration calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Calculate duration of each class\n",
    "dTAGS = {}\n",
    "dTAGS['F'] = [] #F is for fluent\n",
    "dTAGS['D'] = [] #D is for filler\n",
    "dTAGS['O'] = [] #O is for reparandum\n",
    "dTAGS['P'] = [] #P is for repair\n",
    "dTAGS[''] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Parse each sentence, if duration is non-zero, append the duration value \n",
    "    #This is only for words which are not reparandums\n",
    "for k in sent_keys:\n",
    "    for j in SETS[k]:\n",
    "        q = TAGS[TERMINALS[k.split('_')[0]][j][-1]]\n",
    "        s = s + q\n",
    "        cTAGS[q] = cTAGS[q] + 1\n",
    "        \n",
    "        dur = TERMINALS[k.split('_')[0]][j][3] - TERMINALS[k.split('_')[0]][j][2]\n",
    "        \n",
    "        if dur !=0:\n",
    "            if q == 'O' or q == 'P':\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                dTAGS[q].append(dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "#Words which are only reparandums or repair\n",
    "    #Parse each sentence, if duration is non-zero, append the duration value\n",
    "    #It is important to keep this separate because we want to measure the entire length of the reparandum in \n",
    "    #a sentence; This can involved multiple words in a sentence.\n",
    "    \n",
    "for k in X[X['class'] == 'rpdm']['sentence_label'].values:\n",
    "    time = 0\n",
    "    rpdm_time = 0\n",
    "    repair_time = 0\n",
    "    \n",
    "    for j in SETS[k]:\n",
    "        q = TAGS[TERMINALS[k.split('_')[0]][j][-1]]\n",
    "        s = s + q\n",
    "        cTAGS[q] = cTAGS[q] + 1\n",
    "        \n",
    "        dur = TERMINALS[k.split('_')[0]][j][3] - TERMINALS[k.split('_')[0]][j][2]\n",
    "        \n",
    "        if dur !=0:\n",
    "            if q == 'O': # is reparandum\n",
    "                rpdm_time = rpdm_time + dur\n",
    "            if q == 'P': #is repair\n",
    "                repair_time = repair_time + dur\n",
    "      \n",
    "    if rpdm_time!=0:\n",
    "        dTAGS['O'].append(rpdm_time) \n",
    "    if repair_time!=0:\n",
    "        dTAGS['P'].append(repair_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Average fluent duration\n",
    "sum(dTAGS['D'])/len(dTAGS['D'])\n",
    "\n",
    "#Average filler duration\n",
    "sum(dTAGS['F'])/len(dTAGS['F'])\n",
    "\n",
    "#Average reparandum duration\n",
    "sum(dTAGS['O'])/len(dTAGS['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Add prior, duration and sentence probability features to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Prior and duration values calculated from before\n",
    "\n",
    "PRIOR = {}\n",
    "PRIOR['fluent'] = 0.8957749337101879\n",
    "PRIOR['filler'] = 0.07528737810192922\n",
    "PRIOR['rpdm'] = 0.02893768818788287\n",
    "\n",
    "DURATION = {}\n",
    "DURATION['fluent'] = 0\n",
    "DURATION['filler'] = 0.3560657530864195\n",
    "DURATION['rpdm'] = 0.2527699317738784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Usage from https://github.com/simonepri/lm-scorer\n",
    "#To calculate probabilty of sentence from GPT-2\n",
    "import torch\n",
    "from lm_scorer.models.auto import AutoLMScorer as LMScorer\n",
    "\n",
    "# Load model to cpu or cuda\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 1\n",
    "scorer = LMScorer.from_pretrained(\"gpt2\", device=device, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Calculate features based on \n",
    "#     probability of sentence, speaker tag, sentence, sentence include disfluencies, disfluencies\n",
    "PROBS = []\n",
    "LOG_PROBS = []\n",
    "SPEAKER_TAG = []\n",
    "SENTENCE = []\n",
    "DISF_SENTENCE = []\n",
    "DISF = []\n",
    "\n",
    "for i in X['sentence_label'].values:\n",
    "    \n",
    "    spk = i.split('_')[0]\n",
    "    SPEAKER_TAG.append(spk)\n",
    "    \n",
    "    sent = \"\"\n",
    "    disf_sent = \"\"\n",
    "    disf = \"\"\n",
    "    \n",
    "    for j in SETS[i]:\n",
    "        term = TERMINALS[spk][j]\n",
    "        \n",
    "        if term[0] in contractions:\n",
    "            sent = sent[:-1]\n",
    "            disf_sent = disf_sent[:-1]\n",
    "\n",
    "        if term[-1] == 'fluent':\n",
    "            sent = sent + term[0] + \" \"\n",
    "            disf_sent = disf_sent + term[0] + \" \"\n",
    "        \n",
    "        if term[-1] in ('filler','rpdm','rp'):\n",
    "            disf_sent = disf_sent + term[0] + \" \"\n",
    "            disf = disf + term[0] + \" \"\n",
    "        \n",
    "    sent = sent[:-1]\n",
    "    disf_sent = disf_sent[:-1]\n",
    "    disf = disf[:-1]\n",
    "    \n",
    "    \n",
    "    PROBS.append(scorer.sentence_score(sent))\n",
    "    LOG_PROBS.append(scorer.sentence_score(sent,log=True))\n",
    "    SENTENCE.append(sent)\n",
    "    DISF_SENTENCE.append(disf_sent)\n",
    "    DISF.append(disf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Add features based on probability of sentence, speaker tag, sentence, sentence include disfluencies, disfluencies\n",
    "X['sent'] = SENTENCE\n",
    "X['disf_sent'] = DISF_SENTENCE\n",
    "X['disfluent'] = DISF\n",
    "X['speaker'] = SPEAKER_TAG\n",
    "X['probSent'] = PROBS\n",
    "X['logprobSent'] = LOG_PROBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Add speaker features\n",
    "\n",
    "SEX = []\n",
    "AGE = []\n",
    "DIALECT = []\n",
    "for i,j in X.iterrows():\n",
    "    AGE.append(2023 - int(SPEAKER[j['speaker'][2:]][1]))\n",
    "    SEX.append(SPEAKER[j['speaker'][2:]][0])\n",
    "    DIALECT.append(SPEAKER[j['speaker'][2:]][2])\n",
    "\n",
    "X['sex'] = SEX\n",
    "X['age'] = AGE\n",
    "X['dialect'] = DIALECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Remove cases where probability of sentence is 0\n",
    "X = X[X['probSent'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Add RSA probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate S0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "S0 = {}\n",
    "\n",
    "PRIOR = {}\n",
    "PRIOR['fluent'] = 0.8957749337101879\n",
    "PRIOR['filler'] = 0.07528737810192922\n",
    "PRIOR['rpdm'] = 0.02893768818788287\n",
    "\n",
    "\n",
    "for N in range(5,20):\n",
    "    S0[N] = {}\n",
    "    \n",
    "    #Probability of fluent is (p(FLUENT))^N\n",
    "    flu = math.pow(PRIOR['fluent'],N)\n",
    "    \n",
    "    #Probability of filler is (p(FLUENT))^N * (p(FILLER)) * N+1 \n",
    "        #as there are N+1 places the filler can go to\n",
    "    fil = (N+1)*PRIOR['filler']*math.pow(PRIOR['fluent'],N)\n",
    "    \n",
    "    #Probability of reparandum is (p(FLUENT))^N * (p(REPARANDUM)) * N+1 \n",
    "        #as there are N places the reparandum can go to\n",
    "    rep = (N)*PRIOR['rpdm']*math.pow(PRIOR['fluent'],N)\n",
    "    \n",
    "    _sum = flu+fil+rep\n",
    "    \n",
    "    S0[N]['fluent'] = flu/_sum\n",
    "    \n",
    "    S0[N]['filler'] = fil/_sum\n",
    "    \n",
    "    S0[N]['rpdm'] = rep/_sum\n",
    "\n",
    "s0 = []\n",
    "s0_fluent = []\n",
    "s0_filler = []\n",
    "s0_rpdm = []\n",
    "for i,j in X.iterrows():\n",
    "    s0.append(S0[j['length']][j['class']])\n",
    "    s0_fluent.append(S0[j['length']]['fluent'])\n",
    "    s0_filler.append(S0[j['length']]['filler'])\n",
    "    s0_rpdm.append(S0[j['length']]['rpdm'])\n",
    "\n",
    "X['S0'] = s0\n",
    "X['S0_fluent'] = s0_fluent\n",
    "X['S0_filler'] = s0_filler\n",
    "X['S0_rpdm'] = s0_rpdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate L0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# L0 = P(sentence|disfluency) :: S0(disfluency|sentence) * P(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Calculate\n",
    "X['L0'] = X['S0']*X['probSent']\n",
    "X['L0_fluent'] = X['S0_fluent']*X['probSent']\n",
    "X['L0_filler'] = X['S0_filler']*X['probSent']\n",
    "X['L0_rpdm'] = X['S0_rpdm']*X['probSent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Normalise\n",
    "X['L0'] = X['L0']/X['L0'].sum()\n",
    "X['L0_fluent'] = X['L0_fluent']/X['L0_fluent'].sum()\n",
    "X['L0_filler'] = X['L0_filler']/X['L0_filler'].sum()\n",
    "X['L0_rpdm'] = X['L0_rpdm']/X['L0_rpdm'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate P(disfluency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# P(disfluency) = S0(disfluency|sentence) * P(sentence) / L0(sentence|disfluency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Disfprobs = {}\n",
    "\n",
    "Disfprobs['fluent'] = X['S0_fluent'][0]*X['probSent'][0]/X['L0_fluent'][0]\n",
    "Disfprobs['filler'] = X['S0_filler'][0]*X['probSent'][0]/X['L0_filler'][0]\n",
    "Disfprobs['rpdm'] = X['S0_rpdm'][0]*X['probSent'][0]/X['L0_rpdm'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Utility = log(L0) - cost(disfluency)\n",
    "\n",
    "# Cost of disfluency = Average duration of disfluent filler/reparandum\n",
    "\n",
    "# (Should it be average or specific?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DURATION = {}\n",
    "DURATION['fluent'] = 0\n",
    "DURATION['filler'] = -math.log(0.3560657530864195)\n",
    "DURATION['rpdm'] = -math.log(0.2527699317738784)\n",
    "\n",
    "X['S1'] = np.exp(np.log(X['L0'].values) - X['duration'])\n",
    "X['S1_fluent'] = np.exp(np.log(X['L0_fluent'].values) - DURATION['fluent'])\n",
    "X['S1_filler'] = np.exp(np.log(X['L0_filler'].values) - DURATION['filler'])\n",
    "X['S1_rpdm'] = np.exp(np.log(X['L0_rpdm'].values) - DURATION['rpdm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Take sum to normalise\n",
    "_sums = X['S1_filler'] + X['S1_fluent'] + X['S1_rpdm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Normalise\n",
    "X['S1'] = X['S1'] / _sums\n",
    "X['S1_fluent'] = X['S1_fluent'] / _sums\n",
    "X['S1_filler'] = X['S1_filler'] / _sums\n",
    "X['S1_rpdm'] = X['S1_rpdm'] / _sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Utility = log(S0) - cost(sentence)\n",
    "\n",
    "# Cost of sentence = Sentence_Length\n",
    "\n",
    "# (Should it be average or specific?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X['sent_length'] = X['sent'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X['L1'] = np.exp(np.log(X['S0'].values) - np.log(X['sent_length']))\n",
    "X['L1_fluent'] = np.exp(np.log(X['S0_fluent'].values) - np.log(X['sent_length']))\n",
    "X['L1_filler'] = np.exp(np.log(X['S0_filler'].values) - np.log(X['sent_length']))\n",
    "X['L1_rpdm'] = np.exp(np.log(X['S0_rpdm'].values) - np.log(X['sent_length']))\n",
    "\n",
    "#Normalise\n",
    "X['L1'] = X['L1']/X['L1'].sum()\n",
    "X['L1_fluent'] = X['L1_fluent']/X['L1_fluent'].sum()\n",
    "X['L1_filler'] = X['L1_filler']/X['L1_filler'].sum()\n",
    "X['L1_rpdm'] = X['L1_rpdm']/X['L1_rpdm'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# S2(disfluency|sentence) = L1(sentence|disfluency) * P(disfluency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X['disf_prob'] = [Dprobs[x] for x in X['class'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X['S2'] = X['L1']*X['disf_prob']\n",
    "X['S2_fluent'] = X['L1_fluent']*Dprobs['fluent']\n",
    "X['S2_filler'] = X['L1_filler']*Dprobs['filler']\n",
    "X['S2_rpdm'] = X['L1_rpdm']*Dprobs['rpdm']\n",
    "\n",
    "_sums = (X['S2_fluent'] + X['S2_filler'] + X['S2_rpdm'])\n",
    "#Normalise\n",
    "X['S2'] = X['S2']/_sums\n",
    "X['S2_fluent'] = X['S2_fluent']/_sums\n",
    "X['S2_filler'] = X['S2_filler']/_sums\n",
    "X['S2_rpdm'] = X['S2_rpdm']/_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# L2(sentence|disfluency) = S1(disfluency|sentence) * P(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X['L2'] = X['S1']*X['probSent']\n",
    "X['L2_fluent'] = X['S1_fluent']*X['probSent']\n",
    "X['L2_filler'] = X['S1_filler']*X['probSent']\n",
    "X['L2_rpdm'] = X['S1_rpdm']*X['probSent']\n",
    "\n",
    "#Normalise\n",
    "X['L2'] = X['L2']/X['L2'].sum()\n",
    "X['L2_fluent'] = X['L2_fluent']/X['L2_fluent'].sum()\n",
    "X['L2_filler'] = X['L2_filler']/X['L2_filler'].sum()\n",
    "X['L2_rpdm'] = X['L2_rpdm']/X['L2_rpdm'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Analyse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#These are the 3 classes we look at\n",
    "classes = ['fluent','filler','rpdm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### speaker error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Calculate speaker error for different norms\n",
    "    #norm = 2 is squared error\n",
    "def func(norm):\n",
    "    E0 = []\n",
    "    E1 = []\n",
    "    E2 = []\n",
    "    ERROR_S0 = 0\n",
    "    ERROR_S1 = 0\n",
    "    ERROR_S2 = 0\n",
    "\n",
    "    for i,j in X.iterrows():\n",
    "        \n",
    "        e_s0 = ERROR_S0\n",
    "        e_s1 = ERROR_S1\n",
    "        e_s2 = ERROR_S2\n",
    "    \n",
    "        N = norm\n",
    "        for c in classes:\n",
    "            if c == j['class']:\n",
    "                ERROR_S0 = ERROR_S0 + math.pow((1 - j['S0_' + c]),N)\n",
    "                ERROR_S1 = ERROR_S1 + math.pow((1 - j['S1_' + c]),N)\n",
    "                ERROR_S2 = ERROR_S2 + math.pow((1 - j['S2_' + c]),N)\n",
    "\n",
    "            else:\n",
    "                ERROR_S0 = ERROR_S0 + math.pow(j['S0_' + c],N)\n",
    "                ERROR_S1 = ERROR_S1 + math.pow(j['S1_' + c],N)\n",
    "                ERROR_S2 = ERROR_S2 + math.pow(j['S2_' + c],N)\n",
    "                \n",
    "        E0.append(ERROR_S0 - e_s0)\n",
    "        E1.append(ERROR_S1 - e_s1)\n",
    "        E2.append(ERROR_S2 - e_s2)\n",
    "        \n",
    "    X['error_S0_norm'+str(rsaa)] = E0\n",
    "    X['error_S1_norm'+str(rsaa)] = E1\n",
    "    X['error_S2_norm'+str(rsaa)] = E2\n",
    "    print(ERROR_S0,ERROR_S1,ERROR_S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "func(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "func(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "func(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "func(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### speaker accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Calculate the accuracy for different classes\n",
    "MARK_S0 = 0\n",
    "MARK_S1 = 0\n",
    "MARK_S2 = 0\n",
    "\n",
    "PRED_S0 = []\n",
    "PRED_S1 = []\n",
    "PRED_S2 = []\n",
    "MARKERS = []\n",
    "MARKERS_LISTENER = []\n",
    "\n",
    "for i,j in X.iterrows():\n",
    "\n",
    "\n",
    "    t = classes.index(j['class'])\n",
    "\n",
    "    #Store the class with the maximum probability\n",
    "    PRED_S0.append(classes[np.argmax([j['S0_fluent'],j['S0_filler'],j['S0_rpdm']])])\n",
    "    PRED_S1.append(classes[np.argmax([j['S1_fluent'],j['S1_filler'],j['S1_rpdm']])])\n",
    "    PRED_S2.append(classes[np.argmax([j['S2_fluent'],j['S2_filler'],j['S2_rpdm']])])    \n",
    "\n",
    "    #If the maximum probability class is the same as the sentence class give it a MARK (for correct prediction)\n",
    "    if t == np.argmax([j['S0_fluent'],j['S0_filler'],j['S0_rpdm']]):\n",
    "        MARK_S0 = MARK_S0 + 1\n",
    "\n",
    "    if t == np.argmax([j['S1_fluent'],j['S1_filler'],j['S1_rpdm']]):\n",
    "        MARK_S1 = MARK_S1 + 1\n",
    "\n",
    "    if t == np.argmax([j['S2_fluent'],j['S2_filler'],j['S2_rpdm']]):\n",
    "        MARK_S2 = MARK_S2 + 1\n",
    "        \n",
    "    r = ['S0'+'_' + j['class'],'S1'+'_' + j['class'],'S2'+'_' + j['class']]\n",
    "    \n",
    "    #Add the speaker type that predicts the correct class with the highest probability\n",
    "    MARKERS.append('S' + str(np.argmax([j[r[0]],j[r[1]],j[r[2]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#PRED_S tells the most probable class for each sentence predicted by the speaker\n",
    "X['S0_pred'] = PRED_S0\n",
    "X['S1_pred'] = PRED_S1\n",
    "X['S2_pred'] = PRED_S2\n",
    "\n",
    "#MARK tells the speaker type for each sentence\n",
    "    #i.e. the speaker type that predicts the correct class with the highest probability\n",
    "X['MARK'] = MARKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#PRINT ACCURACY\n",
    "print(\"S0:\",sum((X['class'] == X['S0_pred'])*1)/len(X))\n",
    "\n",
    "print(\"S1:\",sum((X['class'] == X['S1_pred'])*1)/len(X))\n",
    "\n",
    "print(\"S2:\",sum((X['class'] == X['S2_pred'])*1)/len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Speaker predictions of what disfluency a sentence should have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(X['S0_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# % distribution\n",
    "x = list(Counter(X['S0_pred']).values())\n",
    "[y/sum(x) for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(X['S1_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# % distribution\n",
    "x = list(Counter(X['S1_pred']).values())\n",
    "[y/sum(x) for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(X['S2_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# % distribution\n",
    "x = list(Counter(X['S2_pred']).values())\n",
    "[y/sum(x) for y in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### comparing fillers of S0 and S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Probability of sentence for S0 filllers\n",
    "X[(X['S0_pred'] == 'filler') & (X['class'] == 'filler')]['probSent'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Probability of sentence for S1 filllers\n",
    "X[(X['S0_pred'] == 'filler') & (X['S1_pred'] == 'filler') & (X['class'] == 'filler')]['probSent'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### listener error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Calculate listener error\n",
    "    #For this we need the ideal listener probabilities (coming from uniform distribution)\n",
    "    #These are:\n",
    "X['Li_fluent'] = (X['class'] == 'fluent')*1/sum((X['class'] == 'fluent')*1)\n",
    "X['Li_filler'] = (X['class'] == 'filler')*1/sum((X['class'] == 'filler')*1)\n",
    "X['Li_rpdm'] = (X['class'] == 'rpdm')*1/sum((X['class'] == 'rpdm')*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#NORM 2, LISTENER SPEAKER ERROR\n",
    "N = 2\n",
    "print(\"Fluent\")\n",
    "print(np.sum(np.power((X['Li_fluent'] - X['L0_fluent']).values,N)))\n",
    "print(np.sum(np.power((X['Li_fluent'] - X['L1_fluent']).values,N)))\n",
    "print(np.sum(np.power((X['Li_fluent'] - X['L2_fluent']).values,N)))\n",
    "print()\n",
    "print(\"Filler\")\n",
    "print(np.sum(np.power((X['Li_filler'] - X['L0_filler']).values,N)))\n",
    "print(np.sum(np.power((X['Li_filler'] - X['L1_filler']).values,N)))\n",
    "print(np.sum(np.power((X['Li_filler'] - X['L2_filler']).values,N)))\n",
    "print()\n",
    "print(\"Rpdm\")\n",
    "print(np.sum(np.power((X['Li_rpdm'] - X['L0_rpdm']).values,N)))\n",
    "print(np.sum(np.power((X['Li_rpdm'] - X['L1_rpdm']).values,N)))\n",
    "print(np.sum(np.power((X['Li_rpdm'] - X['L2_rpdm']).values,N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X['sex_M'] = (X['sex'] == 'M')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dff = X[['S0_filler','S1_filler','S2_filler','length','sent_length','age','sex_M']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dff.columns = ['S0','S1','S2','#words','#characters','age','sex (Male)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(dff, annot=True)\n",
    "# ax.set(xlabel='length', ylabel='count')\n",
    "plt.savefig('speakerType_correlation.png', dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = X[['L0_fluent','L0_filler','L0_rpdm','L1_fluent','L1_filler','L1_rpdm','L2_fluent','L2_filler','L2_rpdm']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['L0 (fluent)','L0 (filler)','L0 (reparandum)','L1 (fluent)','L1 (filler)','L1 (reparandum)','L2 (fluent)','L2 (filler)','L2 (reparandum)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.index = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df, annot=True)\n",
    "# ax.set(xlabel='length', ylabel='count')\n",
    "plt.savefig('list_correlation.png', dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = X[['S0_fluent','S0_filler','S0_rpdm','S1_fluent','S1_filler','S1_rpdm','S2_fluent','S2_filler','S2_rpdm']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['S0 (fluent)','S0 (filler)','S0 (reparandum)','S1 (fluent)','S1 (filler)','S1 (reparandum)','S2 (fluent)','S2 (filler)','S2 (reparandum)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.index = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df, annot=True)\n",
    "# ax.set(xlabel='length', ylabel='count')\n",
    "plt.savefig('spk_correlation.png', dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = X[['L0_fluent','L0_filler','L0_rpdm','L1_fluent','L1_filler','L1_rpdm','L2_fluent','L2_filler','L2_rpdm','S0_fluent','S0_filler','S0_rpdm','S1_fluent','S1_filler','S1_rpdm','S2_fluent','S2_filler','S2_rpdm']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['L0 (fluent)','L0 (filler)','L0 (reparandum)','L1 (fluent)','L1 (filler)','L1 (reparandum)','L2 (fluent)','L2 (filler)','L2 (reparandum)','S0 (fluent)','S0 (filler)','S0 (reparandum)','S1 (fluent)','S1 (filler)','S1 (reparandum)','S2 (fluent)','S2 (filler)','S2 (reparandum)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.index = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Q = df[['S0 (filler)','S0 (reparandum)','S1 (fluent)','S1 (filler)','S1 (reparandum)','S2 (fluent)','S2 (filler)','S2 (reparandum)']][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(Q, annot=True)\n",
    "# ax.set(xlabel='length', ylabel='count')\n",
    "plt.savefig('speakList_correlation.png', dpi=300,bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
